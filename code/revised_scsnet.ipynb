{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#get file\n",
    "f_name = '../data/all_data_s.npy'\n",
    "f_name2 = '../data/all_data_m.npy'\n",
    "\n",
    "np_input1 = np.array(np.load(f_name)).astype(np.float32)\n",
    "np_input2 = np.array(np.load(f_name2)).astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "        0     1     2     3     4     5     6     7     8     9     ...  \\\n0        1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   \n1        1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   \n2        1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   \n3        1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   \n4        1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   \n...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n120955   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n120956   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n120957   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n120958   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n120959   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n\n             1528       1529       1530       1531       1532       1533  \\\n0        0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n1        5.468074   5.542763   5.631670   5.739755   5.873912   6.041899   \n2       10.936148  11.085526  11.263341  11.479509  11.747824  12.083797   \n3       16.404221  16.628290  16.895012  17.219263  17.621735  18.125696   \n4       21.872295  22.171053  22.526682  22.959019  23.495647  24.167595   \n...           ...        ...        ...        ...        ...        ...   \n120955   0.000008   0.000007   0.000007   0.000006   0.000005   0.000004   \n120956   0.000008   0.000008   0.000007   0.000007   0.000006   0.000005   \n120957   0.000009   0.000008   0.000008   0.000007   0.000006   0.000005   \n120958   0.000009   0.000009   0.000008   0.000007   0.000006   0.000005   \n120959   0.000010   0.000009   0.000009   0.000008   0.000007   0.000005   \n\n             1534       1535          1536          1537  \n0        0.000000   0.000000  0.000000e+00  0.000000e+00  \n1        6.244349   6.417093  6.671859e+00  6.289491e+00  \n2       12.488698  12.834187  1.334372e+01  1.257898e+01  \n3       18.733047  19.251280  2.001558e+01  1.886847e+01  \n4       24.977396  25.668373  2.668744e+01  2.515796e+01  \n...           ...        ...           ...           ...  \n120955   0.000003   0.000002  8.122170e-07  2.761359e-07  \n120956   0.000003   0.000002  8.629792e-07  2.933922e-07  \n120957   0.000004   0.000002  9.137431e-07  3.106512e-07  \n120958   0.000004   0.000002  9.645071e-07  3.279102e-07  \n120959   0.000004   0.000002  1.015271e-06  3.451693e-07  \n\n[120960 rows x 1538 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1528</th>\n      <th>1529</th>\n      <th>1530</th>\n      <th>1531</th>\n      <th>1532</th>\n      <th>1533</th>\n      <th>1534</th>\n      <th>1535</th>\n      <th>1536</th>\n      <th>1537</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>5.468074</td>\n      <td>5.542763</td>\n      <td>5.631670</td>\n      <td>5.739755</td>\n      <td>5.873912</td>\n      <td>6.041899</td>\n      <td>6.244349</td>\n      <td>6.417093</td>\n      <td>6.671859e+00</td>\n      <td>6.289491e+00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>10.936148</td>\n      <td>11.085526</td>\n      <td>11.263341</td>\n      <td>11.479509</td>\n      <td>11.747824</td>\n      <td>12.083797</td>\n      <td>12.488698</td>\n      <td>12.834187</td>\n      <td>1.334372e+01</td>\n      <td>1.257898e+01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>16.404221</td>\n      <td>16.628290</td>\n      <td>16.895012</td>\n      <td>17.219263</td>\n      <td>17.621735</td>\n      <td>18.125696</td>\n      <td>18.733047</td>\n      <td>19.251280</td>\n      <td>2.001558e+01</td>\n      <td>1.886847e+01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>21.872295</td>\n      <td>22.171053</td>\n      <td>22.526682</td>\n      <td>22.959019</td>\n      <td>23.495647</td>\n      <td>24.167595</td>\n      <td>24.977396</td>\n      <td>25.668373</td>\n      <td>2.668744e+01</td>\n      <td>2.515796e+01</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>120955</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000008</td>\n      <td>0.000007</td>\n      <td>0.000007</td>\n      <td>0.000006</td>\n      <td>0.000005</td>\n      <td>0.000004</td>\n      <td>0.000003</td>\n      <td>0.000002</td>\n      <td>8.122170e-07</td>\n      <td>2.761359e-07</td>\n    </tr>\n    <tr>\n      <th>120956</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000008</td>\n      <td>0.000008</td>\n      <td>0.000007</td>\n      <td>0.000007</td>\n      <td>0.000006</td>\n      <td>0.000005</td>\n      <td>0.000003</td>\n      <td>0.000002</td>\n      <td>8.629792e-07</td>\n      <td>2.933922e-07</td>\n    </tr>\n    <tr>\n      <th>120957</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000009</td>\n      <td>0.000008</td>\n      <td>0.000008</td>\n      <td>0.000007</td>\n      <td>0.000006</td>\n      <td>0.000005</td>\n      <td>0.000004</td>\n      <td>0.000002</td>\n      <td>9.137431e-07</td>\n      <td>3.106512e-07</td>\n    </tr>\n    <tr>\n      <th>120958</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000009</td>\n      <td>0.000009</td>\n      <td>0.000008</td>\n      <td>0.000007</td>\n      <td>0.000006</td>\n      <td>0.000005</td>\n      <td>0.000004</td>\n      <td>0.000002</td>\n      <td>9.645071e-07</td>\n      <td>3.279102e-07</td>\n    </tr>\n    <tr>\n      <th>120959</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000010</td>\n      <td>0.000009</td>\n      <td>0.000009</td>\n      <td>0.000008</td>\n      <td>0.000007</td>\n      <td>0.000005</td>\n      <td>0.000004</td>\n      <td>0.000002</td>\n      <td>1.015271e-06</td>\n      <td>3.451693e-07</td>\n    </tr>\n  </tbody>\n</table>\n<p>120960 rows × 1538 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(np_input1)\n",
    "df2=pd.DataFrame(np_input2)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "np_input=np_input1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "train_ratio= 0.99\n",
    "sample_rate=1\n",
    "np.random.shuffle(np_input)\n",
    "np_input=np_input[:int(sample_rate * np.shape(np_input)[0]),:]\n",
    "num_train = int(train_ratio * np.shape(np_input)[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18]])"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3,4, 5, 6], [7,8,9,10,11,12],[13,14,15,16,17,18]], np.int32)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12],\n       [13, 14, 15, 16, 17, 18]])"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate=1\n",
    "y=x[:int(sample_rate * np.shape(x)[0]),:]\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[13, 14, 15, 16, 17, 18],\n       [ 1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12]])"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(x)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "x_batch_node = np_data_train[iteration*batch_size:iteration*batch_size+batch_size,0:768]\n",
    "                x_batch_load = np_data_train[iteration*batch_size:iteration*batch_size+batch_size,768:770]\n",
    "                y_batch = np_data_train[iteration*batch_size:iteration*batch_size+batch_size,770:1538]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#np.random.shuffle(np_input)\n",
    "np_input = np_input[:int(sample_rate * np.shape(np_input)[0]), :]\n",
    "\n",
    "num_train = int(train_ratio * np.shape(np_input)[0])\n",
    "# num_test = np.shape(np_input)[0]-num_train\n",
    "\n",
    "np_data_train = np_input[:num_train, :]\n",
    "np_data_test = np_input[num_train:, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\n\\n#Define the 1st CNN layer for Eecoding and Pooling\\nW_conv1 = weight_variable([3,3,1,32])\\nb_conv1 = bias_variable([32])\\nh_conv1 = tf.nn.relu(conv2d(xs_reshape,W_conv1)+b_conv1) #Nonex24x32x32\\nh_pool1 = max_pool_2x2(h_conv1) #Nonex12x16x32\\n\\n #Define the 2nd CNN layer for Eecoding and Pooling\\nW_conv2 = weight_variable([3,3,32,64])\\nb_conv2 = bias_variable([64])\\nh_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2) #Nonex12x16x64\\nh_pool2 = max_pool_2x2(h_conv2) #6x8x64\\n\\n#define FC1\\nW_fc1 = weight_variable([6*8*64, 1024])\\nb_fc1 = bias_variable([1024])\\nh_pool2_flat = tf.reshape(h_pool2, [-1,6*8*64])\\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\\nh_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\\n\\n#define FC2\\nW_fc2 = weight_variable([1024, 30])\\nb_fc2 = bias_variable([30])\\nh_fc2 = tf.nn.softplus(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)  #Nonex30 的向量\\n#h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)  #Nonex30 的向量\\n\\n#Feature representation fully connected\\ndna_full = tf.concat([h_fc2,xs_load],1)\\n#None x32 的向量\\n\\n#Define FC3\\nW_fc3 = weight_variable([32, 1024])\\nb_fc3 = bias_variable([1024])\\nh_fc3 = tf.nn.softplus(tf.matmul(dna_full,W_fc3)+b_fc3) #None x1024 的向量\\n#h_fc3 = tf.nn.relu(tf.matmul(dna_full,W_fc3)+b_fc3) #None x1024 的向量\\n\\n#Define FC4\\nW_fc4 = weight_variable([1024, 6*8*64])\\nb_fc4 = bias_variable([6*8*64])\\nh_fc4 = tf.nn.relu(tf.matmul(h_fc3,W_fc4)+b_fc4) #Nonex3072 的向量\\nh_fc4_drop = tf.nn.dropout(h_fc4,keep_prob)\\n\\n#Define Upsampling and the 3rd CNN layer for Decoding\\n#reshape     debugger breaks\\nh_fc4_drop_flat = tf.reshape(h_fc4_drop, [-1,6,8,64]) #Nonex6x8x64 的向量\\nh_fc4_up = tf.keras.layers.UpSampling2D(size=(2,2))(h_fc4_drop_flat) #Nonex12x16x64\\nW_conv3 = weight_variable([3,3,64,32])\\nb_conv3 = bias_variable([32])\\nh_conv3 = tf.nn.relu(conv2d(h_fc4_up,W_conv3)+b_conv3) #Nonex12x16x32\\n\\n#Define Upsampling and the 4th CNN layer for Decoding\\nh_conv3_up = tf.keras.layers.UpSampling2D(size=(2,2))(h_conv3) #Nonex24x32x32\\nW_conv4 = weight_variable([3,3,32,16])\\nb_conv4 = bias_variable([16])\\nh_conv4 = tf.nn.relu(conv2d(h_conv3_up,W_conv4)+b_conv4) #Nonex24x32x16\\n\\n#Define the 5th CNN layer for Decoding\\nW_conv5 = weight_variable([3,3,16,1])\\nb_conv5 = bias_variable([1])\\nprediction_matrix = tf.nn.relu(conv2d(h_conv4,W_conv5)+b_conv5) #Nonex24x32x1\\nprediction = tf.reshape(prediction_matrix, shape=[-1,resolution])#Nonex768\\n'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "file_name_reference = '../data/all_data_s.npy'\n",
    "all_file_data = np.array(np.load(file_name_reference)).astype(np.float32)\n",
    "\n",
    "\n",
    "# The batch size defines the number of samples that will be propagated through the network. # For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again.\n",
    "batch_size = 256\n",
    "# Step Decay is a learning rate schedule that drops the learning rate by a factor every few epochs\n",
    "# The number of steps between each decay\n",
    "decay_steps = 500\n",
    "# The amount decayed\n",
    "decay_rate = 0.98\n",
    "# Initial learing rate (initial name: starting_learning_rate)\n",
    "initial_learning_rate = 1e-3\n",
    "train_ratio = 0.70\n",
    "n_epochs = 5001\n",
    "# ??? Why have this and 'initial_learning_rate'  (seems to change the number of rows shown, sample_rate=1 shows all rows, any less reduces the rows shown)\n",
    "sample_rate = 1.0\n",
    "\n",
    "#train/test split\n",
    "# This is a single integer equal to the number of rows multiplied to the train_ratio.   This separate the training samples form the testing samples with the ratio given by the train_ratio\n",
    "num_train = int(train_ratio * np.shape(all_file_data)[0])\n",
    "#num_test = np.shape(all_file_data)[0]-num_train\n",
    "\n",
    "# uses train/test split numbers to create 2 numpy arrays with the train and test data respecitively\n",
    "np_data_train = all_file_data[:num_train, :]\n",
    "np_data_test = all_file_data[num_train:, :]\n",
    "\n",
    "#define the size of graph\n",
    "height = 24\n",
    "width = 32\n",
    "resolution = height*width\n",
    "all_file_data = np.array(np.load(file_name_reference)).astype(np.float32)\n",
    "# All of these only use the columns from 770 - 1538\n",
    "stress_mean = np.mean(all_file_data[:,770:1538])\n",
    "stress_min = np.min(all_file_data[:,770:1538])\n",
    "stress_max = np.max(all_file_data[:,770:1538])\n",
    "stree_statistic = [['mean','max','min'],[stress_mean,stress_min,stress_max]]\n",
    "\n",
    "\n",
    "#CNN\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.random.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "#define placeholders for xs,ys,keep_prob\n",
    "xs_node = np_data_train[:,0:768]\n",
    "xs_reshape = tf.reshape(xs_node, shape=[-1,height,width,1])\n",
    "xs_load = np_data_train[:,768:770]\n",
    "ys = np_data_train[:,770:1538]\n",
    "keep_prob = 0.5\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Define the 1st CNN layer for Eecoding and Pooling\n",
    "W_conv1 = weight_variable([3,3,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(xs_reshape,W_conv1)+b_conv1) #Nonex24x32x32\n",
    "h_pool1 = max_pool_2x2(h_conv1) #Nonex12x16x32\n",
    "\n",
    " #Define the 2nd CNN layer for Eecoding and Pooling\n",
    "W_conv2 = weight_variable([3,3,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2) #Nonex12x16x64\n",
    "h_pool2 = max_pool_2x2(h_conv2) #6x8x64\n",
    "\n",
    "#define FC1\n",
    "W_fc1 = weight_variable([6*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,6*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "#define FC2\n",
    "W_fc2 = weight_variable([1024, 30])\n",
    "b_fc2 = bias_variable([30])\n",
    "h_fc2 = tf.nn.softplus(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)  #Nonex30 的向量\n",
    "#h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)  #Nonex30 的向量\n",
    "\n",
    "#Feature representation fully connected\n",
    "dna_full = tf.concat([h_fc2,xs_load],1)\n",
    "#None x32 的向量\n",
    "\n",
    "#Define FC3\n",
    "W_fc3 = weight_variable([32, 1024])\n",
    "b_fc3 = bias_variable([1024])\n",
    "h_fc3 = tf.nn.softplus(tf.matmul(dna_full,W_fc3)+b_fc3) #None x1024 的向量\n",
    "#h_fc3 = tf.nn.relu(tf.matmul(dna_full,W_fc3)+b_fc3) #None x1024 的向量\n",
    "\n",
    "#Define FC4\n",
    "W_fc4 = weight_variable([1024, 6*8*64])\n",
    "b_fc4 = bias_variable([6*8*64])\n",
    "h_fc4 = tf.nn.relu(tf.matmul(h_fc3,W_fc4)+b_fc4) #Nonex3072 的向量\n",
    "h_fc4_drop = tf.nn.dropout(h_fc4,keep_prob)\n",
    "\n",
    "#Define Upsampling and the 3rd CNN layer for Decoding\n",
    "#reshape     debugger breaks\n",
    "h_fc4_drop_flat = tf.reshape(h_fc4_drop, [-1,6,8,64]) #Nonex6x8x64 的向量\n",
    "h_fc4_up = tf.keras.layers.UpSampling2D(size=(2,2))(h_fc4_drop_flat) #Nonex12x16x64\n",
    "W_conv3 = weight_variable([3,3,64,32])\n",
    "b_conv3 = bias_variable([32])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_fc4_up,W_conv3)+b_conv3) #Nonex12x16x32\n",
    "\n",
    "#Define Upsampling and the 4th CNN layer for Decoding\n",
    "h_conv3_up = tf.keras.layers.UpSampling2D(size=(2,2))(h_conv3) #Nonex24x32x32\n",
    "W_conv4 = weight_variable([3,3,32,16])\n",
    "b_conv4 = bias_variable([16])\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3_up,W_conv4)+b_conv4) #Nonex24x32x16\n",
    "\n",
    "#Define the 5th CNN layer for Decoding\n",
    "W_conv5 = weight_variable([3,3,16,1])\n",
    "b_conv5 = bias_variable([1])\n",
    "prediction_matrix = tf.nn.relu(conv2d(h_conv4,W_conv5)+b_conv5) #Nonex24x32x1\n",
    "prediction = tf.reshape(prediction_matrix, shape=[-1,resolution])#Nonex768\n",
    "\"\"\"\n",
    "#\n",
    "# #metrics\n",
    "# mse = tf.losses.mean_squared_error(ys,prediction)\n",
    "# mae = tf.reduce_mean(tf.abs(tf.subtract(ys,prediction)))\n",
    "# #loss\n",
    "# loss = mse\n",
    "#\n",
    "#\n",
    "# #train rate\n",
    "# global_step = tf.Variable(0, trainable=False)\n",
    "# add_global = global_step.assign_add(1)\n",
    "# learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "#                                                global_step=global_step,\n",
    "#                                                decay_steps=decay_steps,decay_rate=decay_rate)\n",
    "# train_step = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#\n",
    "# saver = tf.train.Saver()\n",
    "# start_time = time.localtime()\n",
    "# print('Computing starts at: ', time.strftime('%Y-%m-%d %H:%M:%S', start_time))\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(84672, 768)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_node.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#session\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    test_num = 300\n",
    "    mse_train = np.zeros(n_epochs)\n",
    "    mse_test = np.zeros(n_epochs)\n",
    "    mae_train = np.zeros(n_epochs)\n",
    "    mae_test = np.zeros(n_epochs)\n",
    "    prediction_num = 50\n",
    "    prediction_history = np.zeros((prediction_num, resolution, n_epochs + 1))\n",
    "    prediction_history[:, :, 0] = np_data_test[0:prediction_num, 770:1538]\n",
    "    print('Training...')\n",
    "    for epoch in range(n_epochs):\n",
    "        print(str(epoch) + ' epoch of ' + str(n_epochs))\n",
    "\n",
    "        for iteration in range(num_train // batch_size):\n",
    "            unit_iter = (num_train // batch_size) // 6\n",
    "            if iteration % unit_iter == 0:\n",
    "                print('   ' + str(iteration) + ' iteration of ' + str(num_train // batch_size))\n",
    "            #These are the training and testing stuff        32x24 = 768.\n",
    "            x_batch_node = np_data_train[iteration * batch_size:iteration * batch_size + batch_size, 0:768]\n",
    "            x_batch_load = np_data_train[iteration * batch_size:iteration * batch_size + batch_size, 768:770]\n",
    "            #This is our target stuff     1538-770=768   also a 32x24\n",
    "            y_batch = np_data_train[iteration * batch_size:iteration * batch_size + batch_size, 770:1538]\n",
    "\n",
    "            _, l_rate = sess.run([add_global, learning_rate],\n",
    "                                 feed_dict={xs_node: x_batch_node, xs_load: x_batch_load, ys: y_batch, keep_prob: 0.5})\n",
    "            sess.run(train_step, feed_dict={xs_node: x_batch_node, xs_load: x_batch_load, ys: y_batch, keep_prob: 0.5})\n",
    "\n",
    "        print('Epoch:', epoch, ', Learning rate:', l_rate)\n",
    "\n",
    "        [mse_train[epoch], mae_train[epoch]] = sess.run([mse, mae],\n",
    "                                                        feed_dict={xs_node: x_batch_node, xs_load: x_batch_load,\n",
    "                                                                   ys: y_batch, keep_prob: 0.5})\n",
    "        print('Epoch:', epoch, 'mse_train:', mse_train[epoch])\n",
    "        print('Epoch:', epoch, 'MAE_train:', mae_train[epoch])\n",
    "\n",
    "        [mse_test[epoch], mae_test[epoch], prediction_temp] = sess.run([mse, mae, prediction], feed_dict={\n",
    "            xs_node: np_data_test[0:test_num, 0:768], xs_load: np_data_test[0:test_num, 768:770],\n",
    "            ys: np_data_test[0:test_num, 770:1538], keep_prob: 0.5})\n",
    "        prediction_history[:, :, epoch + 1] = prediction_temp[0:prediction_num, :]\n",
    "        print('Epoch:', epoch, 'mse_test:', mse_test[epoch])\n",
    "        print('Epoch:', epoch, 'MAE_test:', mae_test[epoch])\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        save_path = save_prefix_whereToSave\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        save_file = save_prefix_whereToSave + 'cnn_single'\n",
    "        saver.save(sess, save_file, global_step=epoch, write_meta_graph=True)\n",
    "        print('Session is saved: {}'.format(save_path))\n",
    "\n",
    "        current_time = time.localtime()\n",
    "        print('Time current: ', time.strftime('%Y-%m-%d %H:%M:%S', current_time))\n",
    "\n",
    "    print('Training is finished!')\n",
    "\n",
    "print('Mean stress is:', stress_mean)\n",
    "end_time = time.localtime()\n",
    "print('Computing starts at: ', time.strftime('%Y-%m-%d %H:%M:%S', start_time))\n",
    "print('Time end: ', time.strftime('%Y-%m-%d %H:%M:%S', end_time))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m keep_prob \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241m.\u001B[39mplaceholder(tf\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}